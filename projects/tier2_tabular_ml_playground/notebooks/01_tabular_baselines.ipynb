{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tier 2 — Tabular ML Playground\n",
        "## Starter Notebook: Baselines on California Housing (Regression)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook is the **starter baseline** for the Tier 2 Tabular ML Playground.\n",
        "\n",
        "**Goal:** Load a clean tabular dataset, do quick EDA, train a baseline model, and compare a few classical ML models using consistent metrics.\n",
        "\n",
        "**Dataset:** California Housing (built into scikit-learn)\n",
        "\n",
        "**Models (baseline → stronger):**\n",
        "- Linear Regression (baseline)\n",
        "- Random Forest Regressor\n",
        "- Gradient Boosting Regressor\n",
        "- Support Vector Regressor (with scaling)\n",
        "\n",
        "**Metrics:** RMSE, MAE, R²"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If you get import errors, install requirements from the repo root:\n",
        "# pip install -r requirements.txt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "RANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "housing = fetch_california_housing(as_frame=True)\n",
        "df = housing.frame.copy()\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df.shape, df.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Target and features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "target_col = \"MedHouseVal\"\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Train/test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Metrics helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def regression_metrics(y_true, y_pred):\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Baseline model — Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "lin = LinearRegression()\n",
        "lin.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lin.predict(X_test)\n",
        "baseline = regression_metrics(y_test, y_pred)\n",
        "\n",
        "baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Compare a few classical models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"LinearRegression\": lin,\n",
        "    # Reasonable defaults for a fast baseline comparison (not tuned)\n",
        "    \"RandomForestRegressor\": RandomForestRegressor(\n",
        "        n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1\n",
        "    ),\n",
        "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=RANDOM_STATE),\n",
        "    \"SVR (scaled)\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svr\", SVR(C=10.0, gamma=\"scale\"))\n",
        "    ]),\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    if name != \"LinearRegression\":\n",
        "        model.fit(X_train, y_train)\n",
        "    y_hat = model.predict(X_test)\n",
        "    m = regression_metrics(y_test, y_hat)\n",
        "    results.append({\"model\": name, **m})\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"rmse\")\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Visualize performance (RMSE lower is better)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(results_df[\"model\"], results_df[\"rmse\"])\n",
        "plt.xticks(rotation=25, ha=\"right\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.title(\"Model Comparison (Test RMSE)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Quick reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fill this in after you run the notebook:\n",
        "\n",
        "- Which model had the best RMSE?\n",
        "- Did any model overfit (e.g., suspiciously strong performance vs expectation)?\n",
        "- If you had to pick one model for a first production pass, which would it be and why?\n",
        "- What one change would you try next (feature engineering, cross-validation, tuning, etc.)?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}